{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d615a77",
   "metadata": {},
   "source": [
    "## Ask questions to your PDF\n",
    "\n",
    "This notebook allows you to ask questions to your PDF.\n",
    "\n",
    "we will leverage the power of langchain abstractions to perform the following workflow:\n",
    "\n",
    "### Extract text from a PDF\n",
    "either local or remote files are supported, We will use an UnstructuredPDFLoader to avoid chunking the file twice \n",
    "\n",
    "### Text Splitting\n",
    "When dealing with large text files text splitting is a must to avoid overcapping the token limit of the LLM. We will divide the text into smaller pieces leaving some overlap between them to ensure context is preserved.\n",
    "\n",
    "### Stage our Embedding model\n",
    "Open source Instructor embedding model is one of the most powerful models for text embedding. We will use it to embed our text chunks. this means that we will have a vector representation of each chunk.each vector will be a 768 dimensional vector. that contains te semantic representation of the text.\n",
    "\n",
    "### Vector Database\n",
    "\n",
    "We will use a vector database to store our vectors. this will allow us to perform fast similarity search on our vectors. we will use Pinecone for this.a self-hosted alternative to this can be FAISS\n",
    "Once our pinecone index is created we can store the embeddings created by the embedding model in it.\n",
    "this vectors contain the information of our pdf file.\n",
    "\n",
    "### Question Embedding\n",
    "We will use the same embedding model to embed our question. this will give us a vector representation of our question.With this we can perform a similarity check to get the text chunks that are more similar to our question.\n",
    "This will work as the context for the LLM to predict the answer.\n",
    "\n",
    "### Language Model\n",
    "\n",
    "we will create a chain to perform the following steps:\n",
    "embed a question -> perform a similarity check against the text corpus -> feed the context to the llm -> get a prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b1277",
   "metadata": {},
   "source": [
    "## Project dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58dc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !pip install pypdf langchain huggingface-hub openai transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d3e92ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader\n",
    "from langchain.vectorstores import Chroma, Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import OpenAI,HuggingFaceHub\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import pinecone\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "### Load your PDF file\n",
    "\n",
    "using langchain pdf loader we can load our pdf file. this will return a list of text chunks. each chunk is a string containing the text of the pdf.This will only apply if PyPDFLoader and variants are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b4a2d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(\"./data/constitution.pdf\")\n",
    "\n",
    "##! Other options for loaders\n",
    "#?loader = PyPDFLoader(\"../../data/constitution.pdf\")\n",
    "#?loader = OnlinePDFLoader(\"www.example.com/constitution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcdac23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4fd7c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 52378 characters in your document\n",
      "Content data example (first 100 characters): \n",
      "THE\n",
      "\n",
      "CONSTITUTION of the United States\n",
      "\n",
      "NATIONAL CONSTITUTION CENTER\n",
      "\n",
      "We the People of the United St\n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')\n",
    "\n",
    "print(f'Content data example (first 100 characters): \\n{data[0].page_content[:100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "### Chunk your data up into smaller documents\n",
    "\n",
    "we will use the langchain RecursiveCharacterTextSplitter to split our text into smaller chunks. this will allow us to avoid overcapping the token limit of the LLM. we will use a chunk size of 300 tokens and an overlap of 50 tokens. this means that each chunk will contain 250 tokens of new text and 50 tokens of overlap with the previous chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb3c6f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: If you're using PyPDFLoader then we'll be splitting for the 2nd time.\n",
    "# This is optional, test out on your own data.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "879873a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 254 documents\n"
     ]
    }
   ],
   "source": [
    "print (f'Now you have {len(texts)} documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "## Create embeddings of your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f558b1c",
   "metadata": {},
   "source": [
    "### load env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e093ef3",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv()\n",
    "# Check to see if there is an environment variable with you API keys, if not, use what you put below\n",
    "HUGGINGFACEHUB_API_TOKEN = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc94c8f",
   "metadata": {},
   "source": [
    "### Stage the embedding\n",
    "\n",
    "underneath langchain´s abstractions we will be making api calls to the hugging face inference API or hitting OpenAI endpoints, depending on the model we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e0d1c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'HuggingFaceInstructEmbeddings',\n",
       " 'description': 'Wrapper around sentence_transformers embedding models.\\n\\nTo use, you should have the ``sentence_transformers``\\nand ``InstructorEmbedding`` python packages installed.\\n\\nExample:\\n    .. code-block:: python\\n\\n        from langchain.embeddings import HuggingFaceInstructEmbeddings\\n\\n        model_name = \"hkunlp/instructor-large\"\\n        model_kwargs = {\\'device\\': \\'cpu\\'}\\n        encode_kwargs = {\\'normalize_embeddings\\': True}\\n        hf = HuggingFaceInstructEmbeddings(\\n            model_name=model_name,\\n            model_kwargs=model_kwargs,\\n            encode_kwargs=encode_kwargs\\n        )',\n",
       " 'type': 'object',\n",
       " 'properties': {'client': {'title': 'Client'},\n",
       "  'model_name': {'title': 'Model Name',\n",
       "   'default': 'hkunlp/instructor-large',\n",
       "   'type': 'string'},\n",
       "  'cache_folder': {'title': 'Cache Folder', 'type': 'string'},\n",
       "  'model_kwargs': {'title': 'Model Kwargs', 'type': 'object'},\n",
       "  'encode_kwargs': {'title': 'Encode Kwargs', 'type': 'object'},\n",
       "  'embed_instruction': {'title': 'Embed Instruction',\n",
       "   'default': 'Represent the document for retrieval: ',\n",
       "   'type': 'string'},\n",
       "  'query_instruction': {'title': 'Query Instruction',\n",
       "   'default': 'Represent the question for retrieving supporting documents: ',\n",
       "   'type': 'string'}},\n",
       " 'additionalProperties': False}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d0308",
   "metadata": {},
   "source": [
    "### Initialize Poinecone client \n",
    "\n",
    "create an instance of pinecone client using your credentials and your index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0deb2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV  # next to api key in console\n",
    ")\n",
    "index_name = \"book-questions\" # put in the name of your pinecone index here\n",
    "index = pinecone.Index('book-questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62a0f6",
   "metadata": {},
   "source": [
    "Using langchain Pinecone adapter we can create a reference to the index containing methods to perform queries against the text corpus, the *from_text* method allow us to embedd the text chunks using and existing embedding model and storing it in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "388988ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [t.page_content for t in texts]\n",
    "docsearch = Pinecone.from_texts(texts=text,index_name=index_name, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0512f57e",
   "metadata": {},
   "source": [
    "lets make a test query to see if our index is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34929595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='THE AMENDMENTS TO THE CONSTITUTION OF THE UNITED STATES AS RATIFIED BY THE STATES\\n\\nAmendment I.\\n\\nPreamble to the Bill of Rights', metadata={}), Document(page_content='(Note: The first 10 amendments to the Constitution were ratified December 15, 1791, and form what is known as the “Bill of Rights.”)\\n\\nC O N S T I T U T I O N O F T H E U N I T E D S T A T E S\\n\\nAmendment VI.\\n\\nAmendment XII.', metadata={}), Document(page_content='Passed by Congress June 13, 1866. Ratified July 9, 1868.\\n\\n(Note: Article I, Section 2 of the Constitution was modified by Section 2 of the 14th Amendment.)\\n\\nSECTION 1', metadata={}), Document(page_content='Passed by Congress July 2, 1909. Ratified February 3, 1913.\\n\\n(Note: Article I, Section 9 of the Constitution was modified by the 16 h Amendment.)\\n\\nSECTION 3', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the First Article of the Constitution?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "### Create te context for the LLM\n",
    "langchain´s abstractions allow us to create a chain to perform the following steps despite the llm chosen:\n",
    "embed a question -> perform a similarity check against the text corpus -> feed the context to the llm -> get a prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6b9b1c03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HuggingFaceHub\ntemperature\n  extra fields not permitted (type=value_error.extra)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m llm \u001b[39m=\u001b[39m HuggingFaceHub(repo_id\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtiiuae/falcon-7b-instruct\u001b[39;49m\u001b[39m\"\u001b[39;49m,temperature\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m chain \u001b[39m=\u001b[39m load_qa_chain(llm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/book-questions-XgS9VIsE-python/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/book-questions-XgS9VIsE-python/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HuggingFaceHub\ntemperature\n  extra fields not permitted (type=value_error.extra)"
     ]
    }
   ],
   "source": [
    "#llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f67ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the second article of the constitution about?\"\n",
    "docs = docsearch.similarity_search(query,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3dfd2b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The second article of the Constitution deals with the election of the President and Vice President.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
