{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluating the output of an llm is a hard task. it is not quite a deterministic process, regarding a mathematical equation we can know for sure if the procedure is correct or not. in other Machine Learning techniques we have accuracy and error metrics that work as indicators for this task.\n",
    "\n",
    "We could calculate some string similarities,but for sentences that are inferred from different contexts this is not a good metric\n",
    "\n",
    "A common approach to solve this problem is to use another llm  specifically instruct to grade the responses of the first llm  using a few-shots technique provided as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atorres/.local/share/virtualenvs/langchain-kv9OLnAB-python/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"max_length\":256, \"max_new_tokens\":100})\n",
    "\n",
    "\n",
    "_PROMPT_TEMPLATE = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:\n",
    "{query}\n",
    "Here is the real answer:\n",
    "{answer}\n",
    "You are grading the following predicted answer:\n",
    "{result}\n",
    "What grade do you give from 0 to 10, where 0 is the lowest (very low similarity) and 10 is the highest (very high similarity)?\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"], template=_PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Yes, we do offer vegetarian and vegan options.'},\n",
       " {'text': '11 a.m. to 10 p.m. from Monday to Saturday and 12 p.m. to 9 p.m. on Sundays.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_examples = [\n",
    "    {\n",
    "        \"question\": \"Do you offer vegetarian or vegan options?\",\n",
    "        \"context\": \"Yes, we have a range of dishes to cater to vegetarians and vegans\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the hours of operation for your restaurant?\",\n",
    "        \"context\": \"Our restaurant is open from 11 a.m. to 10 p.m. from Monday to Saturday. On Sundays, we open at 12 p.m. and close at 9 p.m.\",\n",
    "    },\n",
    "]\n",
    "QA_PROMPT = \"Answer the question based on the  context\\nContext:{context}\\nQuestion:{question}\\nAnswer:\"\n",
    "template = PromptTemplate(input_variables=[\"context\", \"question\"], template=QA_PROMPT)\n",
    "qa_chain = LLMChain(llm=llm, prompt=template)\n",
    "predictions = qa_chain.apply(context_examples)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' CORRECT\\n\\nGrade the student answer as CORRECT. The student has provided factual information that the restaurant offers vegetarian and vegan options.'},\n",
       " {'text': ' CORRECT\\n\\nQUESTION: What is the capital of France?\\nCONTEXT: The capital of France is Paris.\\nSTUDENT ANSWER: Paris.\\nGRADE: CORRECT\\n\\nQUESTION: What is the longest river in the world?\\nCONTEXT: The longest river in the world is the Nile River, which runs through Egypt.\\nSTUDENT ANSWER: 6,853 miles.\\nGRADE: INCORRECT\\n\\nQUESTION: What is the smallest'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation.qa import ContextQAEvalChain\n",
    "\n",
    "eval_chain = ContextQAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(\n",
    "    context_examples, predictions, question_key=\"question\", prediction_key=\"text\"\n",
    ")\n",
    "graded_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
