{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Langchain\n",
    "Langchain allows you to interact with your models in a standardized way and lets your easily compose components from langchain to fulfill your special usecase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.0.1-cp311-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "Collecting huggingface-hub\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.0.247-py3-none-any.whl (1.4 MB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-1.25.1-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2023.6.3-cp311-cp311-macosx_11_0_arm64.whl (288 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Using cached tokenizers-0.13.3-cp311-cp311-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Using cached safetensors-0.3.1-cp311-cp311-macosx_12_0_arm64.whl (401 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting fsspec (from huggingface-hub)\n",
      "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached SQLAlchemy-2.0.19-cp311-cp311-macosx_11_0_arm64.whl (2.0 MB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Using cached aiohttp-3.8.5-cp311-cp311-macosx_11_0_arm64.whl (339 kB)\n",
      "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
      "  Using cached langsmith-0.0.15-py3-none-any.whl (30 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain)\n",
      "  Using cached numexpr-2.8.4-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "Collecting pydantic<2,>=1 (from langchain)\n",
      "  Using cached pydantic-1.10.12-cp311-cp311-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Using cached tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached charset_normalizer-3.2.0-cp311-cp311-macosx_11_0_arm64.whl (122 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4-cp311-cp311-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached yarl-1.9.2-cp311-cp311-macosx_11_0_arm64.whl (61 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.0-cp311-cp311-macosx_11_0_arm64.whl (46 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.3-cp311-cp311-macosx_10_9_universal2.whl (17 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tokenizers, safetensors, mpmath, urllib3, typing-extensions, tqdm, tenacity, sympy, regex, pyyaml, python-dotenv, packaging, numpy, networkx, mypy-extensions, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, charset-normalizer, certifi, attrs, async-timeout, yarl, typing-inspect, SQLAlchemy, requests, pydantic, numexpr, marshmallow, jinja2, aiosignal, torch, openapi-schema-pydantic, langsmith, huggingface-hub, dataclasses-json, aiohttp, transformers, langchain\n",
      "Successfully installed MarkupSafe-2.1.3 SQLAlchemy-2.0.19 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.2.0 dataclasses-json-0.5.14 filelock-3.12.2 frozenlist-1.4.0 fsspec-2023.6.0 huggingface-hub-0.16.4 idna-3.4 jinja2-3.1.2 langchain-0.0.247 langsmith-0.0.15 marshmallow-3.20.1 mpmath-1.3.0 multidict-6.0.4 mypy-extensions-1.0.0 networkx-3.1 numexpr-2.8.4 numpy-1.25.1 openapi-schema-pydantic-1.2.4 packaging-23.1 pydantic-1.10.12 python-dotenv-1.0.0 pyyaml-6.0.1 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 sympy-1.12 tenacity-8.2.2 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.31.0 typing-extensions-4.7.1 typing-inspect-0.9.0 urllib3-2.0.4 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv torch transformers huggingface-hub langchain einops bitsandbytes accelerate scipy xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain import  LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lang chain is a powerful abstraction over llm APIs, we don't need to know the specifics of each model's architecture nor the desire request format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"max_length\":64, \"max_new_tokens\":100})\n",
    "llm(\"What is the capital of france?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can create  custom function to implement a chain based on the language model\n",
    "\n",
    "here we will create a promp template to instruct the model on the desire behavior\n",
    "langchain also offers a set of pre configured templates exposed as classes\n",
    "\n",
    "The concept of chains is what makes langchain so powerful, it allows us to create complex behaviors with a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(input_text):\n",
    "    llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"max_length\":64, \"max_new_tokens\":100})\n",
    "    template = \"\"\"\n",
    "You are a helpful bot that answers any question,if you don't know the answer you will ask a human, please do not give false information.\n",
    "Question: {input_text}.\n",
    "\"\"\"\n",
    "    prompt_template = ChatPromptTemplate.from_template(template=template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    return chain.run(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Paris'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chat(\"What is the capital of france?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: The meaning of life is subjective and varies from person to person. It can be a question of finding purpose, happiness, or fulfillment in life. It can also be a spiritual or religious belief. Ultimately, it is up to each individual to determine their own meaning of life.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chat(\"What is the meaning of life?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: The exact number of stars in the observable universe is not known, as it is believed to be infinite. However, it is estimated that there are around 10^67 stars in the observable universe.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = chat(\"tell me the exact number of stars in the observable universe\")\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
