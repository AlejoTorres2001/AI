{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"sentiment\": \"positive\",\\n  \"subject\": \"Pizza Salami\"\\n}'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "Just return the JSON, do not add ANYTHING, NO INTERPRETATION!\n",
    "\n",
    "text: {input}\n",
    "\"\"\"\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"tiiuae/falcon-7b-instruct\", model_kwargs={\"max_length\":256, \"max_new_tokens\":100})\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.predict(input=\"I ordered Pizza Salami and it was awesome!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequentials Chains\n",
    "Sometimes you want to pass the output from one model to a another model. This can be done with different SequentialsChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"\"\"\n",
    "You are a helpful bot that creates 'thank you' response messages based on the sentiment and the subject of the review.You will get a sentiment and subject as inputs to evaluate in json format.\n",
    "\n",
    "If the sentiment is negative, apologize to the customer and offer him a real world assistant to talk to. \n",
    "\n",
    "just return the message you created as plain text.\n",
    "\n",
    "DO NOT INCLUDE ANYTHING ELSE IN YOUR RESPONSE.\n",
    "\n",
    "input: {input}\n",
    "\"\"\"\n",
    "review_template = PromptTemplate(input_variables=[\"input\"], template=response_template)\n",
    "review_chain = LLMChain(llm=llm, prompt=review_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m{\n",
      "  \"sentiment\": \"positive\",\n",
      "  \"subject\": \"Pizza Salami\"\n",
      "}\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "output:\n",
      "\n",
      "Thank you for taking the time to review our product. We appreciate your feedback and are glad to hear that you enjoyed our Pizza Salami. We strive to provide the best quality products and customer service, and we're glad to hear that you had a positive experience with us. If you have any further questions or concerns, please don't hesitate to reach out. We're here to help.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\noutput:\\n\\nThank you for taking the time to review our product. We appreciate your feedback and are glad to hear that you enjoyed our Pizza Salami. We strive to provide the best quality products and customer service, and we're glad to hear that you had a positive experience with us. If you have any further questions or concerns, please don't hesitate to reach out. We're here to help.\""
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, review_chain], verbose=True)\n",
    "\n",
    "overall_chain.run(input=\"I ordered Pizza Salami and was great!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be more complex and not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'Pizza Salami',\n",
       " 'experience': 'It was awful!',\n",
       " 'review': '\"The pizza was disappointing, with a soggy crust and bland toppings. The salami was tough and dry, and the overall taste was unpalatable. I would not recommend this pizza to anyone.\"',\n",
       " 'comment': '\"I\\'m sorry to hear that you didn\\'t enjoy your pizza. We strive to provide the best quality products, and we\\'ll make sure to address any concerns you have. We hope you\\'ll give us another chance to make it right.\"',\n",
       " 'summary': '\\nThe review describes a pizza with a disappointing taste due to a soggy crust, tough and dry toppings, and overall unpalatable taste.'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# This is an LLMChain to write a review given a dish name and the experience.\n",
    "prompt_review = PromptTemplate.from_template(\n",
    "    template=\"You are a helpful bot that creates reviews based on the dish name and the experience. You ordered {dish_name} and your experience was {experience}. Write a review with words: \"\n",
    ")\n",
    "chain_review = LLMChain(llm=llm, prompt=prompt_review, output_key=\"review\")\n",
    "\n",
    "# This is an LLMChain to write a follow-up comment given the restaurant review.\n",
    "prompt_comment = PromptTemplate.from_template(\n",
    "    template=\"You are a helpful bot that creates follow-up comments for reviews.Your comments have to be only one sentence long. Given the review: {review}, write a follow-up comment: \"\n",
    ")\n",
    "chain_comment = LLMChain(llm=llm, prompt=prompt_comment, output_key=\"comment\")\n",
    "\n",
    "# This is an LLMChain to summarize a review.\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    template=\"Summarise the review in one short sentence: \\n\\n {review}\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_review, chain_comment, chain_summary],\n",
    "    verbose=True,\n",
    "    input_variables=[\"dish_name\", \"experience\"],\n",
    "    output_variables=[\"review\", \"comment\", \"summary\",],\n",
    ")\n",
    "\n",
    "overall_chain({\"dish_name\": \"Pizza Salami\", \"experience\": \"It was awful!\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of chaining multiple chains together we can also use an LLM to decide which follow up chain is being used\n",
    "\n",
    "we start by defining 3 templates, a positive, negative and neutral one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "positive_template = \"\"\"You are an AI that focuses on the positive side of things. \\\n",
    "Whenever you analyze a text, you look for the positive aspects and highlight them. \\\n",
    "Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \\\n",
    "not favoring any positive or negative aspects. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \\\n",
    "You analyze a text and show the potential downsides. Here is the text:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are an AI that focuses on the positive side of things. Whenever you analyze a text, you look for the positive aspects and highlight them. your output should be the highlights of the text.\\nHere is the text:\\n{input}', template_format='f-string', validate_template=True), llm=HuggingFaceHub(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text2text-generation/google/flan-t5-xxl', task='text2text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='google/flan-t5-xxl', task=None, model_kwargs={'max_length': 256, 'max_new_tokens': 100}, huggingfacehub_api_token=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}),\n",
       " 'neutral': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, not favoring any positive or negative aspects. Here is the text:\\n{input}', template_format='f-string', validate_template=True), llm=HuggingFaceHub(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text2text-generation/google/flan-t5-xxl', task='text2text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='google/flan-t5-xxl', task=None, model_kwargs={'max_length': 256, 'max_new_tokens': 100}, huggingfacehub_api_token=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}),\n",
       " 'negative': LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='You are an AI that is designed to find the negative aspects in a text. You analyze a text and show the potential downsides. Here is the text:\\n{input}', template_format='f-string', validate_template=True), llm=HuggingFaceHub(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=InferenceAPI(api_url='https://api-inference.huggingface.co/pipeline/text2text-generation/google/flan-t5-xxl', task='text2text-generation', options={'wait_for_model': True, 'use_gpu': False}), repo_id='google/flan-t5-xxl', task=None, model_kwargs={'max_length': 256, 'max_new_tokens': 100}, huggingfacehub_api_token=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={})}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"positive\",\n",
    "        \"description\": \"Good for analyzing positive sentiments\",\n",
    "        \"prompt_template\": positive_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neutral\",\n",
    "        \"description\": \"Good for analyzing neutral sentiments\",\n",
    "        \"prompt_template\": neutral_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"negative\",\n",
    "        \"description\": \"Good for analyzing negative sentiments\",\n",
    "        \"prompt_template\": negative_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"max_length\":256, \"max_new_tokens\":100})\n",
    "\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "destination_chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we have to do some work to get the chain output right.\n",
    "First:\n",
    "We need to create a custom OutPutParser for the router chain, this is due to a small bug in the RouterOutPutParser abstraction in langchain, presumably it has been fixed in newer versions. But in addition to that we have an issue regarding the output format, this is mostly because the free and open source model we are using is not as good as the commercial ones. So we need to do some work to get the output right.\n",
    "The only thing we changed in the parse method is how we load the Json, from te text input that has been previously concatenated to curly braces, just as JSON format needs it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import langchain\n",
    "from typing import Any, Dict, List, Optional, Type, cast\n",
    "\n",
    "class CustomRouterOutputParser(langchain.schema.BaseOutputParser[Dict[str, str]]):\n",
    "    \"\"\"Parser for output of router chain in the multi-prompt chain.\"\"\"\n",
    "\n",
    "    default_destination: str = \"DEFAULT\"\n",
    "    next_inputs_type: Type = str\n",
    "    next_inputs_inner_key: str = \"input\"\n",
    "\n",
    "    def parse(self, text: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            #! bc our model is having a hard time getting json format right, we will just add the curly braces here\n",
    "            if text[-1] != \"}\": text = text + \"}\"\n",
    "            if text[0] != \"{\": text = \"{\" + text\n",
    "            print(\"parsed text\",text)\n",
    "            parsed = json.loads(text)\n",
    "            if not isinstance(parsed[\"destination\"], str):\n",
    "                raise ValueError(\"Expected 'destination' to be a string.\")\n",
    "            if not isinstance(parsed[\"next_inputs\"], self.next_inputs_type):\n",
    "                raise ValueError(\n",
    "                    f\"Expected 'next_inputs' to be {self.next_inputs_type}.\"\n",
    "                )\n",
    "            parsed[\"next_inputs\"] = {self.next_inputs_inner_key: parsed[\"next_inputs\"]}\n",
    "            if (\n",
    "                parsed[\"destination\"].strip().lower()\n",
    "                == self.default_destination.lower()\n",
    "            ):\n",
    "                parsed[\"destination\"] = None\n",
    "            else:\n",
    "                parsed[\"destination\"] = parsed[\"destination\"].strip()\n",
    "            return parsed\n",
    "        except Exception as e:\n",
    "            raise Exception(\n",
    "                f\"Parsing text\\n{text}\\n raised following error:\\n{e}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to dive deep into the Router Template and do some prompt engineering to it.\n",
    "The router Template contains a bunch of instructions and format directives that we need to enhance in order to make the model behave as we want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atorres/.local/share/virtualenvs/langchain-kv9OLnAB-python/lib/python3.11/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed text {\"destination\": \"negative\", \"next_inputs\": \"I ordered Pizza Salami for 9.99$ and I hated it, the crust is awful!\"}\n",
      "negative: {'input': 'I ordered Pizza Salami for 9.99$ and I hated it, the crust is awful!'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The crust is awful'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations) + \"\\n\"\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str).replace(\"<< OUTPUT >>\",\"<< OUTPUT (must only be a json object) >>\").replace(\"\"\"<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}\n",
    "```\"\"\",\"\"\"<< FORMATTING >>\n",
    "Return a JSON object formatted to look like:\n",
    "{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}\n",
    "eg:\n",
    "<< INPUT >>\n",
    "\"What is black body radiation?\"\n",
    "<< OUTPUT >>\n",
    "{{\n",
    "\"destination\": \"physics\",\n",
    "\"next_inputs\": \"What is black body radiation?\"\n",
    "}}\n",
    "\"\"\")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=CustomRouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=destination_chains[\"neutral\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.run(input=\"I ordered Pizza Salami for 9.99$ and I hated it, the crust is awful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
